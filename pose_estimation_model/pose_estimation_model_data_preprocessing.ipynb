{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import json \n",
    "from PIL import Image\n",
    "import random \n",
    "import time \n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS DEFINITIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datapoint:\n",
    "    def __init__(self, metadata_filepath, pose_filepath, rgb_filepath, seg_png_filepath, seg_json_filepath):\n",
    "        # Store the filepaths\n",
    "        self.metadata_filepath = metadata_filepath\n",
    "        self.pose_filepath = pose_filepath\n",
    "        self.rgb_filepath = rgb_filepath\n",
    "        self.seg_png_filepath = seg_png_filepath\n",
    "        self.seg_json_filepath = seg_json_filepath\n",
    "        \n",
    "        # Read the actual data from files and store it\n",
    "        self.metadata = self._read_json(metadata_filepath) if metadata_filepath else None\n",
    "        self.pose = self._read_json(pose_filepath) if pose_filepath else None\n",
    "        self.rgb = self._read_rgb(rgb_filepath) if rgb_filepath else None\n",
    "        self.seg_png = self._read_segmentation_png(seg_png_filepath) if seg_png_filepath else None\n",
    "        self.seg_json = self._read_segmentation_json(seg_json_filepath) if seg_json_filepath else None \n",
    "\n",
    "        # read pose data \n",
    "        self.cam_pose = np.array(self.pose[\"cam\"]).transpose() \n",
    "        self.tag_pose = np.array(self.pose[\"tag\"]).transpose()  \n",
    "        self.tag_pose *= np.array([\n",
    "                            [10,10,10,1],\n",
    "                            [10,10,10,1],\n",
    "                            [10,10,10,1],\n",
    "                            [1,1,1,1]\n",
    "                        ]) # rescale the tag \n",
    "\n",
    "    def _read_json(self, filepath):\n",
    "        \"\"\"Read and parse JSON files.\"\"\"\n",
    "        with open(filepath, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def _read_rgb(self, filepath):\n",
    "        \"\"\"Placeholder for reading RGB image files.\"\"\"\n",
    "        return filepath  # Placeholder: returning the file path to avoid memory overload\n",
    "\n",
    "    def _read_segmentation_png(self, filepath):\n",
    "        \"\"\"Placeholder for reading segmentation PNG image files.\"\"\"\n",
    "        return filepath  # Placeholder: returning the file path to avoid memory overload\n",
    "\n",
    "    def _read_segmentation_json(self, filepath):\n",
    "        \"\"\"Read segmentation JSON files.\"\"\"\n",
    "        with open(filepath, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def compute_diffusion_reflectance(self): \n",
    "        \"\"\"Compute the diffuse reflection based on pose and metadata.\"\"\"\n",
    "        N = np.array(self.pose[\"tag\"])[:3,2] \n",
    "        L = np.array(self.pose[\"light\"])[:3,2] \n",
    "        V = np.array(self.pose[\"cam\"])[:3,2] \n",
    "        light_exposure = self.metadata[\"light\"][\"exposure\"] \n",
    "        I_incident = 2**light_exposure \n",
    "        shininess = 1.0  # Placeholder value \n",
    "        self.diffuse_reflection = I_incident * max(np.dot(N, L), 0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Custom representation for the datapoint object.\"\"\"\n",
    "        return f\"datapoint(metadata_filepath={self.metadata_filepath}, pose_filepath={self.pose_filepath}, rgb_filepath={self.rgb_filepath}, seg_png_filepath={self.seg_png_filepath}, seg_json_filepath={self.seg_json_filepath})\"\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data_folders, out_dir):\n",
    "        self.data_folders = data_folders\n",
    "        self.out_dir = out_dir\n",
    "        self.datapoints = []\n",
    "        self.datapoints_train = []\n",
    "        self.datapoints_val = []\n",
    "\n",
    "    def _get_files_in_subfolder(self, folder, file_extension=None):\n",
    "        \"\"\"Helper method to get files in a subfolder, with an optional file extension filter.\"\"\"\n",
    "        files_list = os.listdir(folder)\n",
    "        if file_extension:\n",
    "            files_list = [file for file in files_list if file.endswith(file_extension)]\n",
    "        # Order files_list by date created\n",
    "        files_list = sorted(files_list, key=lambda x: os.path.getctime(os.path.join(folder, x)))  # Assumes creation dates are synchronized\n",
    "        return files_list\n",
    "\n",
    "    def process_folders(self):\n",
    "        \"\"\"Process the folders and create datapoint objects.\"\"\"\n",
    "        for data_folder in self.data_folders:\n",
    "            metadata_subfolder = os.path.join(data_folder, \"metadata\")\n",
    "            pose_subfolder = os.path.join(data_folder, \"pose\")\n",
    "            rgb_subfolder = os.path.join(data_folder, \"rgb\")\n",
    "            seg_subfolder = os.path.join(data_folder, \"seg\")\n",
    "\n",
    "            # List files in subfolders \n",
    "            metadata_files = self._get_files_in_subfolder(metadata_subfolder, file_extension=\".json\")\n",
    "            pose_files = self._get_files_in_subfolder(pose_subfolder, file_extension=\".json\")\n",
    "            rgb_files = self._get_files_in_subfolder(rgb_subfolder, file_extension=\".png\")\n",
    "            seg_png_files = self._get_files_in_subfolder(seg_subfolder, file_extension=\".png\")\n",
    "            seg_json_files = self._get_files_in_subfolder(seg_subfolder, file_extension=\".json\")\n",
    "\n",
    "            # Make sure the files are indexed and aligned properly (by index) across the subfolders\n",
    "            max_length = max(len(metadata_files), len(pose_files), len(rgb_files), len(seg_png_files), len(seg_json_files))\n",
    "\n",
    "            # Verify that the lengths are the same\n",
    "            if not all(len(files) == max_length for files in [metadata_files, pose_files, rgb_files, seg_png_files, seg_json_files]):\n",
    "                print(f\"Lengths do not match for folder: {data_folder}\")\n",
    "                continue\n",
    "\n",
    "            for i in range(max_length):\n",
    "                # Use index 'i' to fetch corresponding files. If a file doesn't exist, use None.\n",
    "                metadata_filepath = os.path.join(metadata_subfolder, metadata_files[i]) if i < len(metadata_files) else None\n",
    "                pose_filepath = os.path.join(pose_subfolder, pose_files[i]) if i < len(pose_files) else None\n",
    "                rgb_filepath = os.path.join(rgb_subfolder, rgb_files[i]) if i < len(rgb_files) else None\n",
    "                seg_png_filepath = os.path.join(seg_subfolder, seg_png_files[i]) if i < len(seg_png_files) else None\n",
    "                seg_json_filepath = os.path.join(seg_subfolder, seg_json_files[i]) if i < len(seg_json_files) else None\n",
    "\n",
    "                # Create a datapoint object for each corresponding file\n",
    "                data_point = datapoint(metadata_filepath, pose_filepath, rgb_filepath, seg_png_filepath, seg_json_filepath)\n",
    "                self.datapoints.append(data_point)\n",
    "\n",
    "    def get_datapoints(self):\n",
    "        \"\"\"Return the list of datapoint objects.\"\"\"\n",
    "        return self.datapoints\n",
    "    \n",
    "    def get_datapoints_filtered(self):\n",
    "        \"\"\"Return the list of filtered datapoint objects.\"\"\"\n",
    "        return self.datapoints_filtered \n",
    "\n",
    "    def filter_datapoints(self): \n",
    "        \"\"\"Compute the diffusion reflectance and only keep datapoints with positive values.\"\"\"\n",
    "        self.datapoints_filtered = [] \n",
    "        for dp in self.datapoints:\n",
    "            dp.compute_diffusion_reflectance() \n",
    "            if dp.diffuse_reflection > 0: \n",
    "                self.datapoints_filtered.append(dp)\n",
    "\n",
    "    def split_train_val(self, filter=True, frac_train=0.8):\n",
    "        \"\"\"Split the datapoints into training and validation datasets.\"\"\"\n",
    "        if filter: \n",
    "            self.datapoints_train = random.sample(self.datapoints_filtered, int(frac_train * len(self.datapoints_filtered)))\n",
    "            self.datapoints_val = [dp for dp in self.datapoints_filtered if dp not in self.datapoints_train]\n",
    "        else:\n",
    "            self.datapoints_train = random.sample(self.datapoints, int(frac_train * len(self.datapoints)))\n",
    "            self.datapoints_val = [dp for dp in self.datapoints if dp not in self.datapoints_train]\n",
    "\n",
    "    def create_directories(self):\n",
    "        \"\"\"Create directories for training and validation data.\"\"\"\n",
    "        dir_train = os.path.join(self.out_dir, \"train\")\n",
    "        dir_val = os.path.join(self.out_dir, \"val\")\n",
    "        dir_train_rgb = os.path.join(dir_train, \"rgb\")\n",
    "        dir_train_seg = os.path.join(dir_train, \"seg\")\n",
    "        dir_val_rgb = os.path.join(dir_val, \"rgb\")\n",
    "        dir_val_seg = os.path.join(dir_val, \"seg\")\n",
    "\n",
    "        os.makedirs(dir_train_rgb, exist_ok=True)\n",
    "        os.makedirs(dir_train_seg, exist_ok=True)\n",
    "        os.makedirs(dir_val_rgb, exist_ok=True)\n",
    "        os.makedirs(dir_val_seg, exist_ok=True)\n",
    "\n",
    "        return dir_train_rgb, dir_train_seg, dir_val_rgb, dir_val_seg\n",
    "\n",
    "    def preprocess_rgb(self, img_path):  \n",
    "        \"\"\"Preprocess RGB image by resizing it.\"\"\"\n",
    "        new_size = (480, 270)  # Define the new size\n",
    "        img = Image.open(img_path)\n",
    "        img_resized = img.resize(new_size)\n",
    "        return img_resized\n",
    "\n",
    "    def preprocess_seg_img(self, seg_img_path, seg_json_path, tag_seg_color=None):\n",
    "        \"\"\"\n",
    "        Preprocesses the segmentation image by resizing and converting it to a binary mask based on tag color.\n",
    "        \"\"\"\n",
    "        # Validate that the segmentation image file exists\n",
    "        if not os.path.exists(seg_img_path):\n",
    "            raise FileNotFoundError(f\"Segmentation image file not found: {seg_img_path}\")\n",
    "\n",
    "        # Validate that the JSON file exists\n",
    "        if not os.path.exists(seg_json_path):\n",
    "            raise FileNotFoundError(f\"Segmentation JSON file not found: {seg_json_path}\")\n",
    "\n",
    "        # Load the segmentation JSON data if tag_seg_color is not provided\n",
    "        if tag_seg_color is None:\n",
    "            with open(seg_json_path, 'r') as json_file:\n",
    "                seg_json = json.load(json_file)\n",
    "\n",
    "            # Find the tag color from the JSON data\n",
    "            for key, val in seg_json.items(): \n",
    "                if val.get(\"class\") == \"tag0\":  \n",
    "                    # Convert the key (which is a string representing a tuple) into an actual tuple\n",
    "                    tag_seg_color = tuple(map(int, key.strip('()').split(', ')))  # Convert string '(140, 25, 255, 255)' into a tuple (140, 25, 255, 255)\n",
    "                    break\n",
    "            else:\n",
    "                # raise ValueError(\"Tag with class 'tag0' not found in JSON.\")\n",
    "                tag_seg_color = tuple([-1,-1,-1,-1]) # impossible color value # FIXME: this is a workaround which can be turned into something more elegant \n",
    "\n",
    "        # Load and resize the segmentation image\n",
    "        seg_img = Image.open(seg_img_path)\n",
    "        new_size = (480, 270)\n",
    "        seg_img_resized = seg_img.resize(new_size)\n",
    "\n",
    "        # Convert the resized image to a NumPy array\n",
    "        seg_img_resized = np.array(seg_img_resized)\n",
    "\n",
    "        # Check if the image is RGB (3 channels) or RGBA (4 channels) or grayscale (1 channel)\n",
    "        if len(seg_img_resized.shape) == 3:\n",
    "            if seg_img_resized.shape[2] == 3:  # RGB image\n",
    "                # Compare each pixel to the tag color (e.g., RGB triplet)\n",
    "                seg_img_resized = np.all(seg_img_resized == tag_seg_color[:3], axis=-1)  # Create binary mask for RGB image\n",
    "            elif seg_img_resized.shape[2] == 4:  # RGBA image\n",
    "                # Compare each pixel to the tag color (RGBA)\n",
    "                seg_img_resized = np.all(seg_img_resized == tag_seg_color, axis=-1)  # Create binary mask for RGBA image\n",
    "        else:  # If it's a single channel (grayscale), use it directly\n",
    "            seg_img_resized = seg_img_resized == tag_seg_color  # Compare pixel values directly\n",
    "\n",
    "        # Convert the binary mask to uint8 type (0 or 1)\n",
    "        seg_img_resized = (seg_img_resized).astype(np.uint8) * 255  # Multiply by 255 to match image range\n",
    "\n",
    "        # Convert the binary mask back to an image\n",
    "        seg_img_resized = Image.fromarray(seg_img_resized)\n",
    "\n",
    "        return seg_img_resized\n",
    "\n",
    "    def save_preprocessed_images(self, frac_train=0.8):\n",
    "        \"\"\"Loop through train and val datapoints and save preprocessed images and segmentation masks.\"\"\"\n",
    "        dir_train_rgb, dir_train_seg, dir_val_rgb, dir_val_seg = self.create_directories()\n",
    "\n",
    "        for i, dp in enumerate(self.datapoints_train): \n",
    "            img = self.preprocess_rgb(dp.rgb_filepath) \n",
    "            seg = self.preprocess_seg_img(dp.seg_png_filepath, dp.seg_json_filepath) \n",
    "            img.save(os.path.join(dir_train_rgb, f\"img_{i}.png\")) \n",
    "            seg.save(os.path.join(dir_train_seg, f\"seg_{i}.png\"))\n",
    "\n",
    "        for i, dp in enumerate(self.datapoints_val):\n",
    "            img = self.preprocess_rgb(dp.rgb_filepath) \n",
    "            seg = self.preprocess_seg_img(dp.seg_png_filepath, dp.seg_json_filepath) \n",
    "            img.save(os.path.join(dir_val_rgb, f\"img_{i}.png\")) \n",
    "            seg.save(os.path.join(dir_val_seg, f\"seg_{i}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 3\n",
      "Number of filtered datapoints: 3\n"
     ]
    }
   ],
   "source": [
    "# data_folders = [\n",
    "#     \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250225-151250/\", # 4K \n",
    "#     \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250224-105046/\", # 36K \n",
    "#     \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250223-110933/\", # 19K \n",
    "#     \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250222-220540/\", # 16K \n",
    "# ]\n",
    "\n",
    "data_folders = [\n",
    "    \"/media/rp/Elements/abhay_ws/marker_detection_failure_recovery/data/marker_obj_sdg/markers_20250222-220217/\" \n",
    "]\n",
    "\n",
    "# define OUT_DIR based on current date and time \n",
    "# OUT_DIR = f\"/home/anegi/abhay_ws/marker_detection_failure_recovery/segmentation_model/data/data_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "OUT_DIR = os.path.join(data_folders[0], \"out_dir\") \n",
    "os.makedirs(OUT_DIR, exist_ok=True) \n",
    "\n",
    "# Create an instance of the DataProcessor class\n",
    "processor = DataProcessor(data_folders, OUT_DIR)\n",
    "\n",
    "# Process the folders to create the datapoint list\n",
    "processor.process_folders()\n",
    "\n",
    "# Retrieve and print length of the datapoints before and after filtering \n",
    "print(f\"Number of datapoints: {len(processor.datapoints)}\") \n",
    "processor.filter_datapoints() \n",
    "print(f\"Number of filtered datapoints: {len(processor.datapoints_filtered)}\")  \n",
    "\n",
    "# Retrieve filtered datapoints\n",
    "datapoints = processor.get_datapoints_filtered()\n",
    "\n",
    "# Split the datapoints into training and validation sets\n",
    "frac_train = 0.8\n",
    "processor.split_train_val(filter=True, frac_train=frac_train) \n",
    "processor.save_preprocessed_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def world_to_pixel(camera_intrinsics, pose_SE3, world_points):\n",
    "#     \"\"\"\n",
    "#     Converts a list of 3D world points to 2D pixel coordinates using the SE(3) pose and camera intrinsics.\n",
    "\n",
    "#     Parameters:\n",
    "#     - camera_intrinsics: A 3x3 matrix containing the camera intrinsic parameters.\n",
    "#     - pose_SE3: A 4x4 transformation matrix representing the SE(3) pose.\n",
    "#     - world_points: A list of 3D points in world space, where each point is a 3-element array or list [x, y, z].\n",
    "\n",
    "#     Returns:\n",
    "#     - pixel_points: A list of 2D points in pixel space [(x_pixel, y_pixel), ...].\n",
    "#     \"\"\"\n",
    "#     pixel_points = []\n",
    "    \n",
    "#     for world_point in world_points:\n",
    "#         # Convert world point to homogeneous coordinates\n",
    "#         world_point_homogeneous = np.append(world_point, 1)  # Shape (4,)\n",
    "\n",
    "#         # Apply the SE(3) transformation (rotation + translation)\n",
    "#         camera_point_homogeneous = np.dot(pose_SE3, world_point_homogeneous)\n",
    "\n",
    "#         # Extract the 3D camera coordinates (x, y, z)\n",
    "#         x_cam, y_cam, z_cam = camera_point_homogeneous[:3]\n",
    "\n",
    "#         # If the point is behind the camera, skip it\n",
    "#         if z_cam <= 0:\n",
    "#             pixel_points.append(None)  # Add None to indicate invalid point\n",
    "#             continue\n",
    "\n",
    "#         # Project the 3D point onto the 2D image plane\n",
    "#         pixel_point_homogeneous = np.dot(camera_intrinsics, np.array([x_cam, y_cam, z_cam]))\n",
    "\n",
    "#         # Convert homogeneous coordinates to 2D (divide by z_cam for perspective division)\n",
    "#         x_pixel = pixel_point_homogeneous[0] / pixel_point_homogeneous[2]\n",
    "#         y_pixel = pixel_point_homogeneous[1] / pixel_point_homogeneous[2]\n",
    "\n",
    "#         # Add the pixel coordinates to the result list\n",
    "#         pixel_points.append((x_pixel, y_pixel))\n",
    "\n",
    "#     return pixel_points\n",
    "\n",
    "\n",
    "def overlay_points_on_image(image, pixel_points, radius=5, color=(0, 0, 255), thickness=-1):\n",
    "    \"\"\"\n",
    "    Overlays a list of pixel points on the input image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: The input image (a NumPy array).\n",
    "    - pixel_points: A list of 2D pixel coordinates [(x1, y1), (x2, y2), ...].\n",
    "    - radius: The radius of the circle to draw around each point. Default is 5.\n",
    "    - color: The color of the circle (BGR format). Default is red (0, 0, 255).\n",
    "    - thickness: The thickness of the circle. Default is -1 to fill the circle.\n",
    "\n",
    "    Returns:\n",
    "    - The image with points overlaid.\n",
    "    \"\"\"\n",
    "    # Iterate over each pixel point and overlay it on the image\n",
    "    for point in pixel_points:\n",
    "        if point is not None:  # Only overlay valid points\n",
    "            x, y = int(point[0]), int(point[1])\n",
    "            # Draw a filled circle at the pixel coordinates\n",
    "            cv2.circle(image, (x, y), radius, color, thickness)\n",
    "\n",
    "    return image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def marker_to_pixel(camera_intrinsics, pose_SE3, marker_corners_marker_space):\n",
    "    \"\"\"\n",
    "    Converts the marker corners from marker space to image space using camera intrinsics and SE(3) pose.\n",
    "\n",
    "    Parameters:\n",
    "    - camera_intrinsics: A 3x3 camera intrinsic matrix.\n",
    "    - pose_SE3: A 4x4 SE(3) pose matrix of the marker with respect to the camera.\n",
    "    - marker_corners_marker_space: A list of 3D points in marker space (4 points, each [x, y, z]).\n",
    "\n",
    "    Returns:\n",
    "    - marker_corners_image_space: A list of 2D points (x_pixel, y_pixel) in image space.\n",
    "    \"\"\"\n",
    "    \n",
    "    marker_corners_image_space = []\n",
    "\n",
    "    # Loop through each marker corner in marker space\n",
    "    for corner in marker_corners_marker_space:\n",
    "        # Convert corner to homogeneous coordinates (x, y, z, 1)\n",
    "        marker_point_homogeneous = np.append(corner, 1)\n",
    "\n",
    "        # Apply the SE(3) pose transformation to the marker corner\n",
    "        camera_point_homogeneous = np.dot(pose_SE3, marker_point_homogeneous)\n",
    "\n",
    "        # Extract the 3D camera coordinates (x, y, z)\n",
    "        x_cam, y_cam, z_cam = camera_point_homogeneous[:3]\n",
    "\n",
    "        # If the point is behind the camera, skip this point\n",
    "        if z_cam <= 0:\n",
    "            marker_corners_image_space.append(None)\n",
    "            continue\n",
    "\n",
    "        # Project the 3D camera point to 2D image point using camera intrinsics\n",
    "        pixel_point_homogeneous = np.dot(camera_intrinsics, np.array([x_cam, y_cam, z_cam]))\n",
    "\n",
    "        # Convert homogeneous coordinates to 2D (divide by z_cam for perspective division)\n",
    "        x_pixel = pixel_point_homogeneous[0] / pixel_point_homogeneous[2]\n",
    "        y_pixel = pixel_point_homogeneous[1] / pixel_point_homogeneous[2]\n",
    "\n",
    "        # Add the pixel coordinates to the result list\n",
    "        marker_corners_image_space.append((x_pixel, y_pixel))\n",
    "\n",
    "    return marker_corners_image_space\n",
    "\n",
    "dp = processor.datapoints[2] \n",
    "\n",
    "fx = 400 \n",
    "fy = 400 \n",
    "cx = 640/2 \n",
    "cy = 480/2 \n",
    "camera_intrinsics = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])  \n",
    "\n",
    "tf_w_t = dp.tag_pose  \n",
    "tf_c_c = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "# tf_c_c = np.eye(4)  \n",
    "\n",
    "tf_w_c = tf_c_c @ dp.cam_pose  \n",
    "\n",
    "tf_t_t = np.array([\n",
    "    [-1,0,0,0],\n",
    "    [0,0,-1,0],\n",
    "    [0,-1,0,0],\n",
    "    [0,0,0,1] \n",
    "])\n",
    "\n",
    "tf_c_t = np.linalg.inv(tf_w_c) @ tf_t_t @ tf_w_t  \n",
    "\n",
    "# tf_c_t = np.array([\n",
    "#     [1, 0, 0, 0],\n",
    "#     [0, 1, 0, 0],\n",
    "#     [0, 0, 1, 1],\n",
    "#     [0, 0, 0, 1] \n",
    "# ])\n",
    "\n",
    "world_points = [\n",
    "    [0,0,0],\n",
    "    [.1,.1,0],\n",
    "    [-.1,.1,0],\n",
    "    [-.1,-.1,0],\n",
    "    [.1,-.1,0],\n",
    "]\n",
    "\n",
    "marker_corners_marker_space = world_points\n",
    "\n",
    "\n",
    "pixel_points = marker_to_pixel(camera_intrinsics=camera_intrinsics, pose_SE3=tf_c_t, marker_corners_marker_space=marker_corners_marker_space) \n",
    "\n",
    "\n",
    "print(pixel_points) \n",
    "\n",
    "image = cv2.imread(dp.rgb) \n",
    "output_image = overlay_points_on_image(image, pixel_points)\n",
    "\n",
    "# Show the output image with overlaid points\n",
    "cv2.imshow(\"Image with Points\", output_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.15463849  0.98682839  0.04750464]\n",
      " [-0.66287751 -0.13928818  0.73565769]\n",
      " [ 0.73258472  0.08227124  0.67568565]]\n",
      "[ 1.59152025e-16 -3.18304050e-16 -7.16756999e-01]\n"
     ]
    }
   ],
   "source": [
    "print(tf_c_t[:3,:3]) \n",
    "print(tf_c_t[:3,3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.93038066e-32  1.00000000e+00 -2.22044605e-16]\n",
      " [ 2.22044605e-16 -2.22044605e-16 -1.00000000e+00]\n",
      " [-1.00000000e+00  0.00000000e+00 -2.22044605e-16]]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(tf_w_c[:3,:3]) \n",
    "print(tf_w_c[:3,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marker_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
