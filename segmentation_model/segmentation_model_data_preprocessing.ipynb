{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import json \n",
    "from PIL import Image\n",
    "import random \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS DEFINITIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datapoint:\n",
    "    def __init__(self, metadata_filepath, pose_filepath, rgb_filepath, seg_png_filepath, seg_json_filepath):\n",
    "        # Store the filepaths\n",
    "        self.metadata_filepath = metadata_filepath\n",
    "        self.pose_filepath = pose_filepath\n",
    "        self.rgb_filepath = rgb_filepath\n",
    "        self.seg_png_filepath = seg_png_filepath\n",
    "        self.seg_json_filepath = seg_json_filepath\n",
    "        \n",
    "        # Read the actual data from files and store it\n",
    "        self.metadata = self._read_json(metadata_filepath) if metadata_filepath else None\n",
    "        self.pose = self._read_json(pose_filepath) if pose_filepath else None\n",
    "        self.rgb = self._read_rgb(rgb_filepath) if rgb_filepath else None\n",
    "        self.seg_png = self._read_segmentation_png(seg_png_filepath) if seg_png_filepath else None\n",
    "        self.seg_json = self._read_segmentation_json(seg_json_filepath) if seg_json_filepath else None\n",
    "\n",
    "    def _read_json(self, filepath):\n",
    "        \"\"\"Read and parse JSON files.\"\"\"\n",
    "        with open(filepath, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def _read_rgb(self, filepath):\n",
    "        \"\"\"Placeholder for reading RGB image files.\"\"\"\n",
    "        return filepath  # Placeholder: returning the file path to avoid memory overload\n",
    "\n",
    "    def _read_segmentation_png(self, filepath):\n",
    "        \"\"\"Placeholder for reading segmentation PNG image files.\"\"\"\n",
    "        return filepath  # Placeholder: returning the file path to avoid memory overload\n",
    "\n",
    "    def _read_segmentation_json(self, filepath):\n",
    "        \"\"\"Read segmentation JSON files.\"\"\"\n",
    "        with open(filepath, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def compute_diffusion_reflectance(self): \n",
    "        \"\"\"Compute the diffuse reflection based on pose and metadata.\"\"\"\n",
    "        N = np.array(self.pose[\"tag\"])[:3,2] \n",
    "        L = np.array(self.pose[\"light\"])[:3,2] \n",
    "        V = np.array(self.pose[\"cam\"])[:3,2] \n",
    "        light_exposure = self.metadata[\"light\"][\"exposure\"] \n",
    "        I_incident = 2**light_exposure \n",
    "        shininess = 1.0  # Placeholder value \n",
    "        self.diffuse_reflection = I_incident * max(np.dot(N, L), 0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Custom representation for the datapoint object.\"\"\"\n",
    "        return f\"datapoint(metadata_filepath={self.metadata_filepath}, pose_filepath={self.pose_filepath}, rgb_filepath={self.rgb_filepath}, seg_png_filepath={self.seg_png_filepath}, seg_json_filepath={self.seg_json_filepath})\"\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data_folders, out_dir):\n",
    "        self.data_folders = data_folders\n",
    "        self.out_dir = out_dir\n",
    "        self.datapoints = []\n",
    "        self.datapoints_train = []\n",
    "        self.datapoints_val = []\n",
    "\n",
    "    def _get_files_in_subfolder(self, folder, file_extension=None):\n",
    "        \"\"\"Helper method to get files in a subfolder, with an optional file extension filter.\"\"\"\n",
    "        files_list = os.listdir(folder)\n",
    "        if file_extension:\n",
    "            files_list = [file for file in files_list if file.endswith(file_extension)]\n",
    "        # Order files_list by date created\n",
    "        files_list = sorted(files_list, key=lambda x: os.path.getctime(os.path.join(folder, x)))  # Assumes creation dates are synchronized\n",
    "        return files_list\n",
    "\n",
    "    def process_folders(self):\n",
    "        \"\"\"Process the folders and create datapoint objects.\"\"\"\n",
    "        for data_folder in self.data_folders:\n",
    "            metadata_subfolder = os.path.join(data_folder, \"metadata\")\n",
    "            pose_subfolder = os.path.join(data_folder, \"pose\")\n",
    "            rgb_subfolder = os.path.join(data_folder, \"rgb\")\n",
    "            seg_subfolder = os.path.join(data_folder, \"seg\")\n",
    "\n",
    "            # List files in subfolders \n",
    "            metadata_files = self._get_files_in_subfolder(metadata_subfolder, file_extension=\".json\")\n",
    "            pose_files = self._get_files_in_subfolder(pose_subfolder, file_extension=\".json\")\n",
    "            rgb_files = self._get_files_in_subfolder(rgb_subfolder, file_extension=\".png\")\n",
    "            seg_png_files = self._get_files_in_subfolder(seg_subfolder, file_extension=\".png\")\n",
    "            seg_json_files = self._get_files_in_subfolder(seg_subfolder, file_extension=\".json\")\n",
    "\n",
    "            # Make sure the files are indexed and aligned properly (by index) across the subfolders\n",
    "            max_length = max(len(metadata_files), len(pose_files), len(rgb_files), len(seg_png_files), len(seg_json_files))\n",
    "\n",
    "            # Verify that the lengths are the same\n",
    "            if not all(len(files) == max_length for files in [metadata_files, pose_files, rgb_files, seg_png_files, seg_json_files]):\n",
    "                print(f\"Lengths do not match for folder: {data_folder}\")\n",
    "                continue\n",
    "\n",
    "            for i in range(max_length):\n",
    "                # Use index 'i' to fetch corresponding files. If a file doesn't exist, use None.\n",
    "                metadata_filepath = os.path.join(metadata_subfolder, metadata_files[i]) if i < len(metadata_files) else None\n",
    "                pose_filepath = os.path.join(pose_subfolder, pose_files[i]) if i < len(pose_files) else None\n",
    "                rgb_filepath = os.path.join(rgb_subfolder, rgb_files[i]) if i < len(rgb_files) else None\n",
    "                seg_png_filepath = os.path.join(seg_subfolder, seg_png_files[i]) if i < len(seg_png_files) else None\n",
    "                seg_json_filepath = os.path.join(seg_subfolder, seg_json_files[i]) if i < len(seg_json_files) else None\n",
    "\n",
    "                # Create a datapoint object for each corresponding file\n",
    "                data_point = datapoint(metadata_filepath, pose_filepath, rgb_filepath, seg_png_filepath, seg_json_filepath)\n",
    "                self.datapoints.append(data_point)\n",
    "\n",
    "    def get_datapoints(self):\n",
    "        \"\"\"Return the list of datapoint objects.\"\"\"\n",
    "        return self.datapoints\n",
    "    \n",
    "    def get_datapoints_filtered(self):\n",
    "        \"\"\"Return the list of filtered datapoint objects.\"\"\"\n",
    "        return self.datapoints_filtered \n",
    "\n",
    "    def filter_datapoints(self): \n",
    "        \"\"\"Compute the diffusion reflectance and only keep datapoints with positive values.\"\"\"\n",
    "        self.datapoints_filtered = [] \n",
    "        for dp in self.datapoints:\n",
    "            dp.compute_diffusion_reflectance() \n",
    "            if dp.diffuse_reflection > 0: \n",
    "                self.datapoints_filtered.append(dp)\n",
    "\n",
    "    def split_train_val(self, filter=True, frac_train=0.8):\n",
    "        \"\"\"Split the datapoints into training and validation datasets.\"\"\"\n",
    "        if filter: \n",
    "            self.datapoints_train = random.sample(self.datapoints_filtered, int(frac_train * len(self.datapoints_filtered)))\n",
    "            self.datapoints_val = [dp for dp in self.datapoints_filtered if dp not in self.datapoints_train]\n",
    "        else:\n",
    "            self.datapoints_train = random.sample(self.datapoints, int(frac_train * len(self.datapoints)))\n",
    "            self.datapoints_val = [dp for dp in self.datapoints if dp not in self.datapoints_train]\n",
    "\n",
    "    def create_directories(self):\n",
    "        \"\"\"Create directories for training and validation data.\"\"\"\n",
    "        dir_train = os.path.join(self.out_dir, \"train\")\n",
    "        dir_val = os.path.join(self.out_dir, \"val\")\n",
    "        dir_train_rgb = os.path.join(dir_train, \"rgb\")\n",
    "        dir_train_seg = os.path.join(dir_train, \"seg\")\n",
    "        dir_val_rgb = os.path.join(dir_val, \"rgb\")\n",
    "        dir_val_seg = os.path.join(dir_val, \"seg\")\n",
    "\n",
    "        os.makedirs(dir_train_rgb, exist_ok=True)\n",
    "        os.makedirs(dir_train_seg, exist_ok=True)\n",
    "        os.makedirs(dir_val_rgb, exist_ok=True)\n",
    "        os.makedirs(dir_val_seg, exist_ok=True)\n",
    "\n",
    "        return dir_train_rgb, dir_train_seg, dir_val_rgb, dir_val_seg\n",
    "\n",
    "    def preprocess_rgb(self, img_path):  \n",
    "        \"\"\"Preprocess RGB image by resizing it.\"\"\"\n",
    "        new_size = (480, 270)  # Define the new size\n",
    "        img = Image.open(img_path)\n",
    "        img_resized = img.resize(new_size)\n",
    "        return img_resized\n",
    "\n",
    "    def preprocess_seg_img(self, seg_img_path, seg_json_path, tag_seg_color=None):\n",
    "        \"\"\"\n",
    "        Preprocesses the segmentation image by resizing and converting it to a binary mask based on tag color.\n",
    "        \"\"\"\n",
    "        # Validate that the segmentation image file exists\n",
    "        if not os.path.exists(seg_img_path):\n",
    "            raise FileNotFoundError(f\"Segmentation image file not found: {seg_img_path}\")\n",
    "\n",
    "        # Validate that the JSON file exists\n",
    "        if not os.path.exists(seg_json_path):\n",
    "            raise FileNotFoundError(f\"Segmentation JSON file not found: {seg_json_path}\")\n",
    "\n",
    "        # Load the segmentation JSON data if tag_seg_color is not provided\n",
    "        if tag_seg_color is None:\n",
    "            with open(seg_json_path, 'r') as json_file:\n",
    "                seg_json = json.load(json_file)\n",
    "\n",
    "            # Find the tag color from the JSON data\n",
    "            for key, val in seg_json.items(): \n",
    "                if val.get(\"class\") == \"tag0\":  \n",
    "                    # Convert the key (which is a string representing a tuple) into an actual tuple\n",
    "                    tag_seg_color = tuple(map(int, key.strip('()').split(', ')))  # Convert string '(140, 25, 255, 255)' into a tuple (140, 25, 255, 255)\n",
    "                    break\n",
    "            else:\n",
    "                # raise ValueError(\"Tag with class 'tag0' not found in JSON.\")\n",
    "                tag_seg_color = tuple([-1,-1,-1,-1]) # impossible color value # FIXME: this is a workaround which can be turned into something more elegant \n",
    "\n",
    "        # Load and resize the segmentation image\n",
    "        seg_img = Image.open(seg_img_path)\n",
    "        new_size = (480, 270)\n",
    "        seg_img_resized = seg_img.resize(new_size)\n",
    "\n",
    "        # Convert the resized image to a NumPy array\n",
    "        seg_img_resized = np.array(seg_img_resized)\n",
    "\n",
    "        # Check if the image is RGB (3 channels) or RGBA (4 channels) or grayscale (1 channel)\n",
    "        if len(seg_img_resized.shape) == 3:\n",
    "            if seg_img_resized.shape[2] == 3:  # RGB image\n",
    "                # Compare each pixel to the tag color (e.g., RGB triplet)\n",
    "                seg_img_resized = np.all(seg_img_resized == tag_seg_color[:3], axis=-1)  # Create binary mask for RGB image\n",
    "            elif seg_img_resized.shape[2] == 4:  # RGBA image\n",
    "                # Compare each pixel to the tag color (RGBA)\n",
    "                seg_img_resized = np.all(seg_img_resized == tag_seg_color, axis=-1)  # Create binary mask for RGBA image\n",
    "        else:  # If it's a single channel (grayscale), use it directly\n",
    "            seg_img_resized = seg_img_resized == tag_seg_color  # Compare pixel values directly\n",
    "\n",
    "        # Convert the binary mask to uint8 type (0 or 1)\n",
    "        seg_img_resized = (seg_img_resized).astype(np.uint8) * 255  # Multiply by 255 to match image range\n",
    "\n",
    "        # Convert the binary mask back to an image\n",
    "        seg_img_resized = Image.fromarray(seg_img_resized)\n",
    "\n",
    "        return seg_img_resized\n",
    "\n",
    "    def save_preprocessed_images(self, frac_train=0.8):\n",
    "        \"\"\"Loop through train and val datapoints and save preprocessed images and segmentation masks.\"\"\"\n",
    "        dir_train_rgb, dir_train_seg, dir_val_rgb, dir_val_seg = self.create_directories()\n",
    "\n",
    "        for i, dp in enumerate(self.datapoints_train): \n",
    "            img = self.preprocess_rgb(dp.rgb_filepath) \n",
    "            seg = self.preprocess_seg_img(dp.seg_png_filepath, dp.seg_json_filepath) \n",
    "            img.save(os.path.join(dir_train_rgb, f\"img_{i}.png\")) \n",
    "            seg.save(os.path.join(dir_train_seg, f\"seg_{i}.png\"))\n",
    "\n",
    "        for i, dp in enumerate(self.datapoints_val):\n",
    "            img = self.preprocess_rgb(dp.rgb_filepath) \n",
    "            seg = self.preprocess_seg_img(dp.seg_png_filepath, dp.seg_json_filepath) \n",
    "            img.save(os.path.join(dir_val_rgb, f\"img_{i}.png\")) \n",
    "            seg.save(os.path.join(dir_val_seg, f\"seg_{i}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 75771\n",
      "Number of filtered datapoints: 37948\n"
     ]
    }
   ],
   "source": [
    "data_folders = [\n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250225-151250/\", # 4K \n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250224-105046/\", # 36K \n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250223-110933/\", # 19K \n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250222-220540/\", # 16K \n",
    "]\n",
    "\n",
    "# define OUT_DIR based on current date and time \n",
    "OUT_DIR = f\"/home/anegi/abhay_ws/marker_detection_failure_recovery/segmentation_model/data/data_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True) \n",
    "\n",
    "# Create an instance of the DataProcessor class\n",
    "processor = DataProcessor(data_folders, OUT_DIR)\n",
    "\n",
    "# Process the folders to create the datapoint list\n",
    "processor.process_folders()\n",
    "\n",
    "# Retrieve and print length of the datapoints before and after filtering \n",
    "print(f\"Number of datapoints: {len(processor.datapoints)}\") \n",
    "processor.filter_datapoints() \n",
    "print(f\"Number of filtered datapoints: {len(processor.datapoints_filtered)}\")  \n",
    "\n",
    "# Retrieve filtered datapoints\n",
    "datapoints = processor.get_datapoints_filtered()\n",
    "\n",
    "# Split the datapoints into training and validation sets\n",
    "frac_train = 0.8\n",
    "processor.split_train_val(filter=True, frac_train=frac_train) \n",
    "processor.save_preprocessed_images()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marker_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
